{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting ad positioning using Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placement of ads on website is the primary problem for companies that operate on ad revenue. The position where the ad is placed plays pivotal role on whether or not the ad will be clicked. Here we have the following choices:\n",
    "1. Place them randomly, or\n",
    "2. Place the ad on the same position\n",
    "\n",
    "The problem with placing the ad on the same position is the user, after a certain time, will start ignoring the space since he's used to seeing ad at the place, he will end up ignoring that particular position hereafter. Hence, this will reduce the number of clicks on ads. The problem with the former option, placing them randomly, is it wouldn't take optimal positions into consideration. For instance, text beside images are viewed higher number of times than those text which are placed at a distance. It is infeasible to go through every website and repeat the procedure. \n",
    "\n",
    "Solution: Reinforcement Learning\n",
    "\n",
    "Using Reinforcement Learning we can approximate the human behavior. \n",
    "\n",
    "### Why Reinforcement Learning? \n",
    "We cannot use traditional Machine Learning here, since it requires:\n",
    "1. Huge data\n",
    "2. Features\n",
    "3. Tuning of many hyperparameters\n",
    "\n",
    "And we neither have huge data, nor features. The only data we have is the position of the banner/ad and whether or not it was clicked. We will use this dataset from Kaggle: https://www.kaggle.com/akram24/ads-ctr-optimisation. We will solve this problem using both model-based (Policy iteration) and model-free methods(Q-Learning & Monte-Carlo). We'll simplify some assumptions for model-based technique. \n",
    "\n",
    "### Notebook Layout\n",
    "1. MDP environment - Understanding the dataset\n",
    "2. Random Policy - placing the ads randomly on different webpages\n",
    "3. Max policy - placing the ad where it is clicked maximum number of times\n",
    "4. Model-based Method - Policy Iteration\n",
    "5. Model-free Method - Q-learning & Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import routines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "Our environment will be the dataset. It contains 10 ads position per row having values either 1, when the ad is clicked, or 0 when it is not. Every row can be considered as a state in the space, considering it kind of a navigation across multiple pages (on website, for instance) Lets load the dataset and visualize the first few rows.\n",
    "\n",
    "1. state = webpage\n",
    "2. action = placing the ad at any of the 10 positions on a webpage\n",
    "3. reward = +1 if the ad was clicked at the position; else 0\n",
    "4. Transition Probability: next webpage that user will end up in is random; therefore, it is 1/(total_webpages -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ad 1</th>\n",
       "      <th>Ad 2</th>\n",
       "      <th>Ad 3</th>\n",
       "      <th>Ad 4</th>\n",
       "      <th>Ad 5</th>\n",
       "      <th>Ad 6</th>\n",
       "      <th>Ad 7</th>\n",
       "      <th>Ad 8</th>\n",
       "      <th>Ad 9</th>\n",
       "      <th>Ad 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
       "0     1     0     0     0     1     0     0     0     1      0\n",
       "1     0     0     0     0     0     0     0     0     1      0\n",
       "2     0     0     0     0     0     0     0     0     0      0\n",
       "3     0     1     0     0     0     0     0     1     0      0\n",
       "4     0     0     0     0     0     0     0     0     0      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = pd.read_csv('Ads_CTR_Optimisation.csv')\n",
    "env.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random policy\n",
    "\n",
    "If we were to not have Reinforcement Learning, we would place the ads randomly at given positions. We will now simulate the same.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward collected: 1277\n"
     ]
    }
   ],
   "source": [
    "# total rewards earned\n",
    "reward = 0\n",
    "# random policy: for every state, choose a random\n",
    "# position for displaying the ad\n",
    "for x in range(len(env)):\n",
    "    action = np.random.randint(0, 10)\n",
    "    # if the guess was correct, increase the reward\n",
    "    if env.values[x][action] == 1:\n",
    "        reward += 1\n",
    "print(\"Reward collected: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Max Policy\n",
    "Another question we might ask: is it right to display the ad where it is clicked the most number of times. For instance, there might be a certain position where the ad clicked with a higher probability. Since the values of the rows is either 1 or 0, we can sum across the columns and count the number of times ad in the position was clicked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    counts\n",
       "ad        \n",
       "1     1703\n",
       "2     1295\n",
       "3      728\n",
       "4     1196\n",
       "5     2695\n",
       "6      126\n",
       "7     1112\n",
       "8     2091\n",
       "9      952\n",
       "10     489"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicked_counts = env.values.sum(axis=0)\n",
    "counts = pd.DataFrame({\"ad\": np.arange(1, 11), \"counts\": clicked_counts})\n",
    "counts.set_index(\"ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which indicates ad 5 was clicked 2695 times. So if we were to always place an ad on position 5, it would be click around 2695 times. But can we do better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming (Policy Iteration) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting with random policy, choose a random choice for every state in the environment\n",
    "state_size = len(env)\n",
    "\n",
    "state_list = []\n",
    "for state in range(state_size):\n",
    "    state_list.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode: 100\n",
      "Current episode: 200\n",
      "Current episode: 300\n",
      "Current episode: 400\n",
      "Current episode: 500\n",
      "Time taken in seconds:  1304.556524515152\n",
      "Total episodes trained: 501\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\r\n",
    "# action could be placing the ad on any of the 10 positions\r\n",
    "action_space = np.arange(0, 10)\r\n",
    "\r\n",
    "policy = [random.choice(action_space) for x in range(state_size)]\r\n",
    "# will take random action for the first time\r\n",
    "first_time = True\r\n",
    "\r\n",
    "# delta\r\n",
    "small_change = 1e-20\r\n",
    "# discount factor\r\n",
    "gamma = 0.9\r\n",
    "episodes = 0\r\n",
    "max_episodes = 500\r\n",
    "\r\n",
    "V = dict()\r\n",
    "# last positions reward will be 1 - terminal state\r\n",
    "V[10000] = 1\r\n",
    "\r\n",
    "# initially the value function for all states will be random values close to zero\r\n",
    "for i in range(state_size):\r\n",
    "    V[i] = np.random.random()\r\n",
    "deltas = []\r\n",
    "while episodes < max_episodes:\r\n",
    "    # policy evaluation (until convergence of state value function)\r\n",
    "    while True:\r\n",
    "        if episodes > max_episodes:\r\n",
    "            break\r\n",
    "        episodes += 1\r\n",
    "        if episodes % 100 == 0:\r\n",
    "            print(\"Current episode: {}\".format(episodes))\r\n",
    "        biggest_change = 0\r\n",
    "        # loop through every state present\r\n",
    "        for state in range(state_size):\r\n",
    "            old_V = V[state]\r\n",
    "            # take random action according to policy\r\n",
    "            action = policy[state]\r\n",
    "            #print(action)\r\n",
    "            new_state = random.choice(list(set(state_list) - set([state])))\r\n",
    "            #print(new_state)\r\n",
    "            reward = env.values[state][action]\r\n",
    "            #\r\n",
    "            V[state] = (reward + gamma * V[new_state])/9999\r\n",
    "            # We're calculating biggest change to have an idea on convergence. \r\n",
    "            # Initially, the changes will be huge, but as \r\n",
    "            # we update the values, they will tend towards a convergence point\r\n",
    "            biggest_change = max(biggest_change, abs(V[state] - old_V))\r\n",
    "        deltas.append(biggest_change)\r\n",
    "        if biggest_change < small_change:\r\n",
    "            break\r\n",
    "            \r\n",
    "    # policy improvement\r\n",
    "    policy_changed = False\r\n",
    "    for state in range(state_size):\r\n",
    "        best_val = -np.inf\r\n",
    "        best_action = -1\r\n",
    "        for action in action_space:\r\n",
    "            new_state = random.choice(list(set(state_list) - set([state])))\r\n",
    "            reward = env.values[state][action]\r\n",
    "            # calculate the action with the best\r\n",
    "            # future reward\r\n",
    "            future_reward = (reward + gamma * V[new_state])/9999\r\n",
    "            if future_reward > best_val:\r\n",
    "                best_val = future_reward\r\n",
    "                best_action = action\r\n",
    "        assert best_action != -1\r\n",
    "        # After convergence, the policy will not change since we would have already reached\r\n",
    "        # the optimum policy. So check if the policy is not updated, if not then stop. \r\n",
    "        if policy[state] != best_action:\r\n",
    "            policy_changed = True\r\n",
    "        policy[state] = best_action\r\n",
    "\r\n",
    "    if not policy_changed:\r\n",
    "        break\r\n",
    "\r\n",
    "end = time.time()\r\n",
    "print(\"Time taken in seconds: \", end-start)\r\n",
    "print(\"Total episodes trained: {}\".format(episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "plt.plot(deltas, label=\"deltas\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence Plot - Dynamic Programming (Policy Iteration)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward collected: 7424\n"
     ]
    }
   ],
   "source": [
    "# total rewards earned\n",
    "reward = 0\n",
    "# random policy: for every state, choose a random\n",
    "# position for displaying the ad\n",
    "for x in range(len(env)):\n",
    "    action = policy[x]\n",
    "    # if the guess was correct, increase the reward\n",
    "    if env.values[x][action] == 1:\n",
    "        reward += 1\n",
    "print(\"Reward collected: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Q-Learning\n",
    "\n",
    "The reason for using Q-Learning here is :\n",
    "1. It is model free, so it doesn't require to know all the states.\n",
    "2. Intuitive to understand, and converges faster. \n",
    "\n",
    "We will use Q-Learning with Epsilon Decay. Initially, we will start with a very high epsilon value. Which would make the model explore, instead of exploit. After a few iterations, we will reduce this \"exploration probability\" so the model will exploit, instead of exploring. After many iterations, the model would have high confidence and no more exploration is necessary. \n",
    "\n",
    "This strategy is called as \"epsilon greedy\". We will use a decay rate to change the epsilon value such that it becomes lower after many iterations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using q-learning\n",
    "states = len(env)\n",
    "actions = 10\n",
    "# initialize q-table with zeros. Initially all the q-values will be zero\n",
    "q_table = np.zeros((states + 1, actions))\n",
    "\n",
    "learning_rate = 0.7\n",
    "gamma = 0.618\n",
    "\n",
    "# set the exploration probability to be very high initially. \n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01\n",
    "max_episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploit(eps):\n",
    "    \"\"\"Randomizes a number to select\n",
    "    whether or not to expolit\"\"\"\n",
    "    return np.random.uniform() > eps\n",
    "\n",
    "def random_action():\n",
    "    return np.random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 50\n",
      "Episode: 100\n",
      "Episode: 150\n",
      "Episode: 200\n",
      "Episode: 250\n",
      "Episode: 300\n",
      "Episode: 350\n",
      "Episode: 400\n",
      "Episode: 450\n",
      "Episode: 500\n"
     ]
    }
   ],
   "source": [
    "deltas = []\n",
    "reward = 0\n",
    "for episode in range(max_episodes + 1):\n",
    "    if episode % 50 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "    biggest_change = 0\n",
    "    for state in range(states):\n",
    "        if exploit(epsilon):\n",
    "            action = random_action()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        r = env.values[state][action]\n",
    "        reward += r\n",
    "        old_q = q_table[state][action]\n",
    "        new_state = random.choice(list(set(state_list) - set([state])))\n",
    "        q_table[state][action] += learning_rate*(r + gamma*np.max(q_table[new_state, :]) - \n",
    "                                                 q_table[state][action])\n",
    "        biggest_change = max(biggest_change, np.abs(q_table[state][action] - old_q))\n",
    "    # keep track of biggest changes\n",
    "    deltas.append(biggest_change)\n",
    "    # epsilon decay to reduce exploration and increase exploitation\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deltas, label=\"deltas\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence Plot - Q-Learning\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "rewards = 0\n",
    "for state in range(states):\n",
    "    best_action = np.argmax(q_table[state, :])\n",
    "    r = env.values[state][best_action]\n",
    "    rewards += r\n",
    "print(\"Reward collected: {}\".format(rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"Returns a random available choice\"\"\"\n",
    "    return np.random.randint(0, 10)\n",
    "\n",
    "def simulate(policy):\n",
    "    # store the reward-state-action triple\n",
    "    rsa = []\n",
    "    for s in range(env.shape[0]):\n",
    "        # 10% probability that the agent will act randomly\n",
    "        if np.random.uniform() < 0.1:\n",
    "            action = np.random.randint(0, 10)\n",
    "        else:\n",
    "            action = policy[s]\n",
    "        reward = env.values[s][action]\n",
    "        rsa.append((reward, s, action))\n",
    "        \n",
    "    # go reverse and distribute rewards\n",
    "    G = 0\n",
    "    gamma = 0.3\n",
    "    first = True\n",
    "    state_returns = []\n",
    "    for reward, s , a in reversed(rsa):\n",
    "        G = G + gamma * reward\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        else:\n",
    "            state_returns.append((s, a, G))\n",
    "    # we want the rewards to be the way they were distributed\n",
    "    state_returns.reverse()\n",
    "    return state_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# creates a random policy for every timestep\n",
    "policy = {s: np.random.randint(0, 10) for s in range(10000)}\n",
    "Q = np.zeros(env.values.shape)\n",
    "returns = defaultdict(list)\n",
    "deltas = []\n",
    "max_iterations = 2000\n",
    "for x in range(max_iterations):\n",
    "    if x % 100 == 0:\n",
    "        print(x)\n",
    "    # simulate an episode, and get the\n",
    "    # reward-state-action triple\n",
    "    state_returns = simulate(policy)\n",
    "    change = 0.0\n",
    "    for state, action, G in state_returns:\n",
    "        old_q = Q[state][action]\n",
    "        returns[(state, action)].append(G)\n",
    "        # calculates the average returns of being in this state\n",
    "        Q[state][action] = np.mean(returns[(state, action)])\n",
    "        # save the change so we can later on visualize the rate of convergence\n",
    "        change = np.maximum(change, np.abs(Q[state][action] - old_q).astype(np.int))\n",
    "    deltas.append(change)\n",
    "    for s in range(10000):\n",
    "        policy[s] = np.argmax(Q[s, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(deltas, label=\"deltas\")\n",
    "plt.legend()\n",
    "plt.title(\"Convergence Plot - Monte Carlo\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once the policy is learnt, we will test it on the actual environment\n",
    "reward = 0\n",
    "for s in range(10000 - 1):\n",
    "    a = np.argmax(Q[s, :])\n",
    "    reward += env.values[s][a]\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0820f791f01ee903193bd2368737a2812e34ecd4c9a46712bc02fd0a198af90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3  ('tensorflow_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}