{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from collections import deque\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory_model = 'save_model'\n",
    "directory_graph = 'save_graph'\n",
    "\n",
    "# #directory = 'save_graph'\n",
    "# if not os.path.exists(directory_model):\n",
    "#     os.makedirs(directory_model)\n",
    "\n",
    "# if not os.path.exists(directory_graph):\n",
    "#     os.makedirs(directory_graph)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "Please make the value of epsilon to 0 while testing the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        # If you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "    \n",
    "        \n",
    "        self.batch_size = 32\n",
    "        self.train_start = 500\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=1000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "\n",
    "        #if self.load_model:\n",
    "        self.model.load_weights(\"./save_model/cartpole_dqn.h5\")\n",
    "        \n",
    "        # make the epsilon value 0 when testing the model\n",
    "        self.epsilon = 0.0\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        '''\n",
    "        TODO:\n",
    "        Build multilayer perceptron to train the Q(s,a) function. In this neural network, the input will be states and the output \n",
    "        will be Q(s,a) for each (state,action). \n",
    "        Note: Since the ouput Q(s,a) is not restricted from 0 to 1, we use 'linear activation' as output layer.\n",
    "\n",
    "        Loss Function:\n",
    "        Loss=1/2 * (R_t + γ∗max Q_t (S_{t+1},a)−Q_t(S_t,a)^2\n",
    "               which is 'mean squared error'\n",
    "\n",
    "        '''\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='linear',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def save_model_graph(self):\n",
    "        # serialize model to JSON\n",
    "        model_json = self.model.to_json()\n",
    "        with open(\"./save_model/cartpole_dqn_model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        '''\n",
    "        TODO:\n",
    "        Update the target Q-value network to current Q-value network after training for a episode. This means that weights an\n",
    "        biases of target Q-value network will become same as current Q-value network.\n",
    "        '''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        '''\n",
    "        Select action\n",
    "        Args:\n",
    "            state: At any given state, choose action\n",
    "        \n",
    "        TODO:\n",
    "        Choose action according to ε-greedy policy. We generate a random number over [0, 1) from uniform distribution.\n",
    "        If the generated number is less than ε, we will explore, otherwise we will exploit the policy by choosing the\n",
    "        action which has maximum Q-value.\n",
    "        \n",
    "        More the ε value, more will be exploration and less exploitation.\n",
    "        \n",
    "        '''\n",
    "        # choose random action if generated random number is less than ε.\n",
    "        # Action is represented by index, 0-Number of actions, like (0,1,2,3) for 4 actions\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        # if generated random number is greater than ε, choose the action which has max Q-value\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        '''\n",
    "        Save sample in memory and decay ε after we generate each sample from environment. \n",
    "        \n",
    "        Args:\n",
    "            (state, action, reward, next_state, done)- <s,a,r,s',done> \n",
    "        \n",
    "        TODO:\n",
    "            We are saving each sample  (state, action, reward, next_state, done) of the episode, in a memory. Memory can be \n",
    "            defined by queue. We will dequeue sample of batch size from the memory and use it to train the neural network.\n",
    "            \n",
    "            ε-decay:\n",
    "            With ε, we explore and with 1-ε, we exploit. Initially we want to explore more, but at later point, after training \n",
    "            the model, we have good policy to choose better action. So, at that point, we want to expoit more and explore less.\n",
    "            So, we want to decrease the value of ε, by which we explore. \n",
    "            \n",
    "            self.epsilon_min:\n",
    "            Minimum value of ε, by which we want to explore. If the current value of ε is greater then \n",
    "            minimum value to ε, we will decay ε gradually, when generating samples. \n",
    "            \n",
    "            Note: The rate by which we will decrease ε should be slow, otherwise we will not explore much and instead settle\n",
    "            for suboptimal policy instead of optiomal policy. \n",
    "    \n",
    "        '''\n",
    "        # Adding sample to the memory. \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Decay in ε after we generate each sample from the environment\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        '''\n",
    "        Train the neural network to find the best policy\n",
    "        \n",
    "        TODO:\n",
    "        1. Sample <s,a,r,s',done> of batch size from the memory\n",
    "        2. Set the target as R_t + γ∗max Q_t(S_{t+1},a)−Q_t(S_t,a)\n",
    "        3. Set the target only for the action we took in the environment. For the other actions, we don't wan't to \n",
    "        update the network. \n",
    "        4. Remember that we already the actions that we took when generating sample from environment\n",
    "        4. To find the Q_t(S_{t+1},a), we input the next state s' to the model, and we get Q-value for all the actions\n",
    "        5. To find the Q_t(S_t,a), we input the current state s to the model, and we get Q-value for all the actions\n",
    "        6. Train the model\n",
    "        \n",
    "        Note:\n",
    "        We use 2 different neural network for Q_t(S_t,a) and target Q_t(S_{t+1},a). This is so because we are \n",
    "        constantly updating the current Q-value network at each and every timestep in a episode. Therefore, the target \n",
    "        Q-value will change subsequently. The network can become destabilized by falling into feedback loops between the\n",
    "        target and current Q-values.\n",
    "        We update the target Q-value network only after completion of a batch. We update the target Q-value with the \n",
    "        current Q-value network. \n",
    "        \n",
    "        '''\n",
    "        # We start the training only when we have sufficient sample in the memory. We set the number of samples required\n",
    "        # start training in variable train_start\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from the memory\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "     \n",
    "        # Initialise the variables update_input and update_target for a batch for storing the s and s'.\n",
    "        # Later, we will use it to store Q_t(S_t,a_t) and Q_t(S_{t+1},a)\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "\n",
    "        # Set the values of input, action, reward, target and done using memory\n",
    "        # Note the order of <s,a,r,s',done> \n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        # Set the target as Q values predicted from the current state and next state \n",
    "        # store Q_t(S_t,a_t) and Q_t(S_{t+1},a) in target and target_val\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.target_model.predict(update_target)\n",
    "        \n",
    "        \n",
    "        # Update the target value according to the update policy of Q-learning\n",
    "        # R_t + γ ∗ max Q_t(S_{t+1},a)−Q_t(S_t,a_t)\n",
    "        for i in range(self.batch_size):\n",
    "            # Q Learning: get maximum Q value at s' from target model\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.discount_factor * (\n",
    "                    np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.fit(update_input, target, batch_size=self.batch_size,\n",
    "                       epochs=1, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model\n",
    "Test the model by running only for one episode. Most of the time, the score will be 500, sometimes, but sometimes it may be less. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "The score is: 500.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "done = False\n",
    "score = 0\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "while not done:\n",
    "\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    score += reward\n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        print('The score is:', score)\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
