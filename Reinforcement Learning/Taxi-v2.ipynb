{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving OpenAI Gym Environment - (Taxi-v2)\n",
    "\n",
    "In this Python demo, we'll try solving the classic cab-driver problem. The purpose of this notebook is to show how to solve OpenAI Gym environments. We'll demonstrate Q-learning & SARSA on the Taxi environment.\n",
    "\n",
    "Let's now look at the problem statement\n",
    "\n",
    "Here, the objective is to pick up the passenger from one position and drop them off at another in minimum possible time. For this problem, we'll consider our environment to be a 5x5 grid. \n",
    "\n",
    "<img src=\"cab_problem.png\" style=\"width: 300px;\">\n",
    "Image source: https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "\n",
    "There are 4 locations (R, G, Y, B) marked in the image. And the task is to pick up the passenger from one of the four locations and drop him off at other. There is a reward of +20 for a successful dropoff, and -1 for every timestep it takes and -10 for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import routines\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "current state is : 332\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\") # Create environment\r\n",
    "\r\n",
    "state = env.reset()\r\n",
    "env.render()  # helps in visualizing the environment\r\n",
    "\r\n",
    "print(\"current state is :\" ,state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rendering:\n",
    "    - yellow: taxi is unoccupied\n",
    "    - green: taxi is occupied by a passenger\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - other grids: locations\n",
    "    \n",
    "That means, for now, I'm at yellow box and need to pick the passenger from 'G' and drop him off at 'B'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "\n",
    "The state vector for this problem is (col_index, row_index, destination_locations, passenger_position)\n",
    "There are 5 rows, 5 columns and 4 destination locations. What about the passenger locations? 4 or 5?\n",
    "\n",
    "If the passenger is not in cab that means he could be only at one of the four locations. But we also need to account for 1 addition state if the passenger is inside the cab. So, passenger could be at any 4+1 possible locations.\n",
    "\n",
    "Therefore, the state space = 5x5x4x5 = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space :  500\n"
     ]
    }
   ],
   "source": [
    "# Number of possible states\n",
    "state_size = env.observation_space.n \n",
    "print(\"State space : \", state_size)\n",
    "#print(\"Current state : \" ,env.env.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "\n",
    "At any state, the cab driver can either move in any of the four directions or it can pickup/ drop (legally or illegally)\n",
    "\n",
    "    - 0: south\n",
    "    - 1: north\n",
    "    - 2: east\n",
    "    - 3: west\n",
    "    - 4: pickup\n",
    "    - 5: drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space :  6\n"
     ]
    }
   ],
   "source": [
    "# Number of possible actions\n",
    "action_size = env.action_space.n \n",
    "print(\"Action space : \", action_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Let's know solve the given MDP using Q-learning & SARSA.\n",
    "\n",
    "### Q-Learning\n",
    "Q-Learning is an off-policy optimal control algorithm. It learns the Q-values by taking the next action based on the greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialise q-table with zeros\n",
    "Q_table = np.zeros((state_size, action_size))\n",
    "print(Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100000        # Total episodes       \n",
    "\n",
    "#hyperparameters\n",
    "learning_rate = 0.1      # Learning rate\n",
    "gamma = 0.8              # discount factor\n",
    "epsilon = 0.1            # exploration -exploitation tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the policy epsilon-greedy\n",
    "def epsilon_greedy(state, table):\n",
    "    z = np.random.random() # Randomizes a number to select whether or not to expolit\n",
    "    \n",
    "    if z > epsilon:\n",
    "        action = np.argmax(table[state])    #Exploitation: this gets the action corresponding to max q-value of current state\n",
    "    else:\n",
    "        action = env.action_space.sample()    #Exploration: randomly choosing and action\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  2.978031873703003\n",
      "maximum difference:  2.740579541082866e-09\n"
     ]
    }
   ],
   "source": [
    "start = time.time()    # tracking time\n",
    "deltas = []\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset() # Reset the environment\n",
    "    done = False        # 'done' defines successfully dropping the passenger off; \n",
    "                        # resulting in an end of episode\n",
    "    step = 0\n",
    "    biggest_change = 0  # to keep a track of difference in the Q-values\n",
    "    \n",
    "    if episode % 5000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        action = epsilon_greedy(state, Q_table)\n",
    "        \n",
    "        # Take the action and observe the new state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        oldQ_table = Q_table[state, action]\n",
    "                \n",
    "        # UPDATE RULE\n",
    "        Q_table[state, action] += learning_rate * (reward + gamma * \n",
    "                                                   np.max(Q_table[new_state,:])-Q_table[state,action])\n",
    "        \n",
    "        biggest_change = max(biggest_change, np.abs(Q_table[state][action] - oldQ_table))\n",
    "        \n",
    "        state = new_state\n",
    "                             \n",
    "    deltas.append(biggest_change)\n",
    "    \n",
    "    if deltas[-1] < 0.00000001:\n",
    "        break\n",
    "        \n",
    "    episode += 1\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "training_time = end - start\n",
    "print(\"Time taken in seconds: \", training_time)\n",
    "print(\"maximum difference: \", deltas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.89861525, -3.8674188 , -3.89971934, -3.90507106, -6.03654487,\n",
       "       -7.11564551])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_table[454]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Q-Table\n",
    "\n",
    "Let's know test our Q-learning agent on a different environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's change the environment\n",
    "state = env.reset()  # reset will set the environment to a new and random state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "Episode Reward =  12\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while(done == False):\n",
    "    \n",
    "    best_action = np.argmax(Q_table[state,:]) # selecting the best action basis Q-table\n",
    "    \n",
    "    # Take the best action and observe the new state and reward\n",
    "    state, reward, done, info = env.step(best_action) \n",
    "    cumulative_reward += reward  \n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print('Episode Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA\n",
    "\n",
    "SARSA is on-policy learning algorithm. Unlinke Q-learning, it learns the Q-values by taking the next action based on the current policy rather than a greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "current state is : 433\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()  # helps in visualizing the environment\n",
    "\n",
    "print(\"current state is :\" ,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialise sarsa-table with zeros\n",
    "Sarsa_table = np.zeros((state_size, action_size))\n",
    "print(Sarsa_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "Episode: 20000\n",
      "Episode: 30000\n",
      "Episode: 40000\n",
      "Episode: 50000\n",
      "Episode: 60000\n",
      "Episode: 70000\n",
      "Episode: 80000\n",
      "Episode: 90000\n",
      "Episode: 100000\n",
      "Episode: 110000\n",
      "Episode: 120000\n",
      "Episode: 130000\n",
      "Episode: 140000\n",
      "Episode: 150000\n",
      "Episode: 160000\n",
      "Episode: 170000\n",
      "Episode: 180000\n",
      "Episode: 190000\n",
      "Episode: 200000\n",
      "Episode: 210000\n",
      "Episode: 220000\n",
      "Episode: 230000\n",
      "Episode: 240000\n",
      "Episode: 250000\n",
      "Episode: 260000\n",
      "Episode: 270000\n",
      "Episode: 280000\n",
      "Episode: 290000\n",
      "Episode: 300000\n",
      "Episode: 310000\n",
      "Episode: 320000\n",
      "Episode: 330000\n",
      "Episode: 340000\n",
      "Episode: 350000\n",
      "Episode: 360000\n",
      "Episode: 370000\n",
      "Episode: 380000\n",
      "Episode: 390000\n",
      "Episode: 400000\n",
      "Episode: 410000\n",
      "Episode: 420000\n",
      "Episode: 430000\n",
      "Episode: 440000\n",
      "Episode: 450000\n",
      "Episode: 460000\n",
      "Episode: 470000\n",
      "Episode: 480000\n",
      "Episode: 490000\n",
      "Episode: 500000\n",
      "Episode: 510000\n",
      "Episode: 520000\n",
      "Episode: 530000\n",
      "Episode: 540000\n",
      "Episode: 550000\n",
      "Episode: 560000\n",
      "Episode: 570000\n",
      "Episode: 580000\n",
      "Episode: 590000\n",
      "Episode: 600000\n",
      "Episode: 610000\n",
      "Episode: 620000\n",
      "Episode: 630000\n",
      "Episode: 640000\n",
      "Episode: 650000\n",
      "Episode: 660000\n",
      "Episode: 670000\n",
      "Episode: 680000\n",
      "Episode: 690000\n",
      "Episode: 700000\n",
      "Episode: 710000\n",
      "Episode: 720000\n",
      "Episode: 730000\n",
      "Episode: 740000\n",
      "Episode: 750000\n",
      "Episode: 760000\n",
      "Episode: 770000\n",
      "Episode: 780000\n",
      "Episode: 790000\n",
      "Episode: 800000\n",
      "Time taken in seconds:  3010.5816378593445\n",
      "maximum difference:  1.2314036841560672\n"
     ]
    }
   ],
   "source": [
    "episodes = 800000\n",
    "start = time.time()    # tracking time\n",
    "deltas = []\n",
    "for episode in range(1,episodes+1):\n",
    "    \n",
    "    state = env.reset() # Reset the environment\n",
    "    done = False        # 'done' defines successfully dropping the passenger off; \n",
    "                        # resulting in an end of episode\n",
    "    step = 0\n",
    "    biggest_change = 0  # to keep track of difference in Q-values\n",
    "    \n",
    "    if episode % 10000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "        \n",
    "    while not done:\n",
    "\n",
    "        action = epsilon_greedy(state, Sarsa_table)\n",
    "        \n",
    "        # Take the action and observe the new state and reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # Get the action basis epsilon greedy policy\n",
    "        next_action = epsilon_greedy(next_state, Sarsa_table)\n",
    "        \n",
    "        oldSarsa_table = Sarsa_table[state, action]\n",
    "        \n",
    "        # UPDATE RULE\n",
    "        Sarsa_table[state, action] += learning_rate * (reward + gamma * Sarsa_table[next_state,next_action] \n",
    "                                                       -Sarsa_table[state,action])\n",
    "        \n",
    "        biggest_change = max(biggest_change, np.abs(Sarsa_table[state][action] \n",
    "                                                    - oldSarsa_table))\n",
    "        \n",
    "        state = new_state\n",
    "                             \n",
    "    deltas.append(biggest_change)\n",
    "    \n",
    "    if deltas[-1] < 0.00000001:\n",
    "        break\n",
    "        \n",
    "    episode += 1\n",
    "\n",
    "    \n",
    "end = time.time()\n",
    "training_time = end - start\n",
    "print(\"Time taken in seconds: \", training_time)\n",
    "print(\"maximum difference: \", deltas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ -2.79553249,  -2.00891947,  -2.90367588,  -2.14340376,\n",
       "         -1.        , -10.77917148],\n",
       "       [ -2.52554156,  -1.78743424,  -2.83075438,  -2.1287967 ,\n",
       "         -1.        , -10.86776799],\n",
       "       ...,\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sarsa_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.45411306,  -4.83709314,  -5.59351909,  -5.54876487,\n",
       "       -14.40686644, -13.53769902])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sarsa_table[263]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the SARSA table\n",
    "\n",
    "Let's know test our SARSA agent on a different environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's change the environment\n",
    "state = env.reset()  # reset will set the environment to a new and random state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode Reward =  -200\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while(done == False):\n",
    "    \n",
    "    best_action = np.argmax(Sarsa_table[state,:]) # selecting the best action basis Sarsa-table\n",
    "    \n",
    "    # Take the best action and observe the new state and reward\n",
    "    state, reward, done, info = env.step(best_action) \n",
    "    cumulative_reward += reward  \n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print('Episode Reward = ', cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "66529af4e939b7bcbc41896c5279ef4fbdc287bfb9e46b131d571964c5c60b3e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('upgradpy': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}