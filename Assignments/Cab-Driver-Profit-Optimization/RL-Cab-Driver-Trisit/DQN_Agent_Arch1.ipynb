{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DQN_Agent_Arch1.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Cab-Driver Agent"
      ],
      "metadata": {
        "id": "wTe98pGWVu-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# Importing libraries\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import math\r\n",
        "from collections import deque\r\n",
        "import collections\r\n",
        "import pickle\r\n",
        "import time\r\n",
        "\r\n",
        "# for building DQN model\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "\r\n",
        "# for plotting graphs\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Import the environment\r\n",
        "from Env import CabDriver"
      ],
      "outputs": [],
      "metadata": {
        "id": "fxln8-QcVu-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining Time Matrix"
      ],
      "metadata": {
        "id": "50W1RQxQVu-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "# Loading the time matrix provided\r\n",
        "Time_matrix = np.load(\"TM.npy\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "ERYaz9OQVu-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tracking the state-action pairs for checking convergence\n"
      ],
      "metadata": {
        "id": "uv6QA1pbVu-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "States_track = collections.defaultdict(dict)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XOzBRylVVu_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# Below are the list of state-action pairs that are tracked for convergence\r\n",
        "sample_q_values = [\r\n",
        "    ((1, 2, 4), (1, 2)),\r\n",
        "    ((1, 8, 2), (1, 0))\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "# Initialise states-action dictonary to be tracked for convergence\r\n",
        "def initialise_tracking_states():\r\n",
        "    for q_values in sample_q_values:\r\n",
        "        States_track[q_values[0]][q_values[1]] = []\r\n",
        "\r\n",
        "\r\n",
        "# let initialise the States_track dictornary\r\n",
        "initialise_tracking_states()"
      ],
      "outputs": [],
      "metadata": {
        "id": "reIcQbkmVu_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# function to add q-value for sampled q-value state-action pair\r\n",
        "def save_tracking_states(env, agent, state_size):\r\n",
        "    for state in States_track.keys():\r\n",
        "        for action in States_track[state].keys():\r\n",
        "            q_values = agent.model.predict(np.array(env.state_encod_arch1(state)).reshape(1, state_size))\r\n",
        "            q_value = np.take(q_values[0], env.action_space.index(action))\r\n",
        "            States_track[state][action].append(q_value)"
      ],
      "outputs": [],
      "metadata": {
        "id": "piZioB72Vu_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "#Defining a function to save the Q-dictionary as a pickle file\r\n",
        "def save_obj(obj, name ):\r\n",
        "    with open(name + '.pkl', 'wb') as f:\r\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Rk-xhlJmVu_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Class\n",
        "\n",
        "If you are using this framework, you need to fill the following to complete the following code block:\n",
        "1. State and Action Size\n",
        "2. Hyperparameters\n",
        "3. Create a neural-network model in function 'build_model()'\n",
        "4. Define epsilon-greedy strategy in function 'get_action()'\n",
        "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
        "6. Complete the 'train_model()' function with following logic:\n",
        "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
        "      - Initialise your input and output batch for training the model\n",
        "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
        "      - Get Q(s', a) values from the last trained model\n",
        "      - Update the input batch as your encoded state and output batch as your Q-values\n",
        "      - Then fit your DQN model using the updated input and output batch."
      ],
      "metadata": {
        "id": "Vk1MZUsSVu_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "class DQNAgent():\r\n",
        "    def __init__(self, environment, action_size, state_size, hyperparameter):\r\n",
        "        # Define size of state and action\r\n",
        "        self.environment = environment\r\n",
        "        self.state_size = state_size\r\n",
        "        self.action_size = action_size\r\n",
        "\r\n",
        "        # hyper parameters for the DQN\r\n",
        "        self.discount_factor = hyperparameter['discount_factor']\r\n",
        "        self.learning_rate = hyperparameter['learning_rate']\r\n",
        "        self.epsilon = hyperparameter['epsilon']\r\n",
        "        self.epsilon_max = hyperparameter['epsilon_max']\r\n",
        "        self.epsilon_decay = hyperparameter['epsilon_decay']\r\n",
        "        self.epsilon_min = hyperparameter['epsilon_min']\r\n",
        "        \r\n",
        "        self.batch_size = hyperparameter['batch_size']     \r\n",
        "        # create replay memory using deque\r\n",
        "        self.memory = deque(maxlen=2000)\r\n",
        "\r\n",
        "        # create main model and target model\r\n",
        "        self.model = self.build_model()\r\n",
        "\r\n",
        "    \r\n",
        "    def set_epsilon(self, episode):\r\n",
        "        \"\"\"Method to set new epsilon based on current episode\"\"\"\r\n",
        "        self.epsilon = self.epsilon_min + (\r\n",
        "            self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay * episode)\r\n",
        "        return self.epsilon\r\n",
        "\r\n",
        "    def get_epsilon(self):\r\n",
        "        \"\"\"Return epsilon\"\"\"\r\n",
        "        return self.epsilon\r\n",
        "\r\n",
        "\r\n",
        "    # approximate Q function using Neural Network\r\n",
        "    def build_model(self):\r\n",
        "        \"\"\"Build a model\"\"\"\r\n",
        "        model = Sequential()\r\n",
        "\r\n",
        "        # hidden layers\r\n",
        "        model.add(Dense(50, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "        # model.add(Dense(36, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "        # model.add(Dense(36, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "\r\n",
        "        # the output layer: output is of size num_actions\r\n",
        "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\r\n",
        "\r\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\r\n",
        "\r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "    def get_action(self, state):\r\n",
        "        \"\"\"\r\n",
        "        get action in a state according to an epsilon-greedy approach\r\n",
        "        \"\"\"\r\n",
        "        # get possible action index and actions from a given state \r\n",
        "        (possible_actions_index, actions) = self.environment.requests(state)\r\n",
        "\r\n",
        "        # based on epsilon-greedy, either randomaly choose an action or max q-value action\r\n",
        "        if random.uniform(self.epsilon_max, self.epsilon_min) < self.get_epsilon(): \r\n",
        "            # randomly choose the action index and corrosponding action\r\n",
        "            index = random.randrange(len(possible_actions_index))\r\n",
        "            return (possible_actions_index[index], actions[index])\r\n",
        "\r\n",
        "        else:\r\n",
        "            # convert given state to vector and get q-value vector\r\n",
        "            state = np.array(self.environment.state_encod_arch1(state)).reshape(1, self.state_size)\r\n",
        "            q_value = self.model.predict(state)\r\n",
        "            \r\n",
        "            # filter q-values for a possible action index\r\n",
        "            filter_q_value = np.take(q_value[0], possible_actions_index)\r\n",
        "            \r\n",
        "            # check if offline action has higher q-value than accepting a drive request\r\n",
        "            if q_value[0][0] > np.max(filter_q_value):\r\n",
        "                # action index and action of offline\r\n",
        "                return (0, actions[-1])\r\n",
        "            else:\r\n",
        "                # return max q-value action index and action\r\n",
        "                return (possible_actions_index[np.argmax(filter_q_value)], actions[np.argmax(filter_q_value)])\r\n",
        "\r\n",
        "\r\n",
        "    def append_sample(self, state, action, reward, next_state, done):\r\n",
        "        # append the tuple (s, a, r, s', done) to memory (replay buffer) after every action\r\n",
        "        self.memory.append((state, action, reward, next_state, done))\r\n",
        "\r\n",
        "\r\n",
        "    def train_model(self):\r\n",
        "        \"\"\"\r\n",
        "        train the neural network on a minibatch. Input to the network is the states,\r\n",
        "        output is the target q-value corresponding to each action.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        if len(self.memory) > self.batch_size:\r\n",
        "            \r\n",
        "            # sample minibatch from memory\r\n",
        "            minibatch = random.sample(self.memory, self.batch_size)\r\n",
        "\r\n",
        "            # initialise two matrices - update_input and update_output\r\n",
        "            update_input = np.zeros((self.batch_size, self.state_size))\r\n",
        "            update_output = np.zeros((self.batch_size, self.state_size))\r\n",
        "            actions, rewards, done = [], [], []\r\n",
        "\r\n",
        "            # populate update_input and update_output and the lists rewards, actions, done\r\n",
        "            for i in range(self.batch_size):\r\n",
        "                state, action, reward, next_state, done_boolean = minibatch[i]\r\n",
        "                update_input[i] = self.environment.state_encod_arch1(state)\r\n",
        "                actions.append(action)\r\n",
        "                rewards.append(reward)\r\n",
        "                update_output[i] = self.environment.state_encod_arch1(next_state)\r\n",
        "                done.append(done_boolean)\r\n",
        "\r\n",
        "            # predict the target q-values from states s\r\n",
        "            target = self.model.predict(update_input)\r\n",
        "\r\n",
        "            # target for q-network\r\n",
        "            target_qval = self.model.predict(update_output)\r\n",
        "\r\n",
        "            # update the target values\r\n",
        "            for i in range(self.batch_size):\r\n",
        "                if done[i]:\r\n",
        "                    target[i][actions[i]] = rewards[i]\r\n",
        "                else: # non-terminal state\r\n",
        "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\r\n",
        "\r\n",
        "            # model fit\r\n",
        "            history = self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\r\n",
        "\r\n",
        "            # return neural network loss\r\n",
        "            return history.history['loss']\r\n",
        "\r\n",
        "\r\n",
        "    def save_model_weights(self, name):\r\n",
        "        self.model.save_weights(name)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "qn5Ha5oCVu_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "056XR7iLVu_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "Episodes = 5000\r\n",
        "\r\n",
        "max_car_drive_time = 30 * 24\r\n",
        "\r\n",
        "# hyperparameter of Agent class\r\n",
        "Hyperparameters = {\r\n",
        "    'discount_factor': 0.95,\r\n",
        "    'learning_rate' : 0.06,\r\n",
        "    'epsilon': 1,\r\n",
        "    'epsilon_max' : 1,\r\n",
        "    'epsilon_decay' : 0.0005,\r\n",
        "    'epsilon_min': 0.01,\r\n",
        "    'batch_size' : 256\r\n",
        "}\r\n",
        "\r\n",
        "# state size will no. of locations, no. of days and no. of days in a week. = 36\r\n",
        "state_size = 36\r\n",
        "\r\n",
        "# initialise CabDriver Environment\r\n",
        "env = CabDriver()\r\n",
        "# get action space from environment\r\n",
        "(action_space, _, _) = env.reset()\r\n",
        "\r\n",
        "action_size = len(action_space)\r\n",
        "\r\n",
        "# initialise the DQNAgent with action_size, state_size and hyperparameter defined above\r\n",
        "agent = DQNAgent(environment= env, action_size= action_size, state_size= state_size, hyperparameter= Hyperparameters)\r\n",
        "\r\n",
        "# below variable are used for tracking rewards and neural network loss\r\n",
        "(rewards_per_episode, nn_loss_per_episode) = ([],[])"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn7-oIFbVu_G",
        "outputId": "7a04246d-990c-47c6-b248-4adadd2c9521"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Training"
      ],
      "metadata": {
        "id": "UPib6p1uVu_H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "!pip install tqdm"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in d:\\repositories\\upgrad-pgd-ml---ai\\tensorflow_env\\lib\\site-packages (4.60.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: You are using pip version 21.1; however, version 21.1.3 is available.\n",
            "You should consider upgrading via the 'd:\\repositories\\upgrad-pgd-ml---ai\\tensorflow_env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "from tqdm import trange\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "\r\n",
        "for episode in trange(Episodes, desc= 'Episodes'):\r\n",
        "    \r\n",
        "    done = False\r\n",
        "    total_reward = 0\r\n",
        "    agent_total_time = 0\r\n",
        "    \r\n",
        "    (_, _, curr_state) = env.reset()\r\n",
        "    \r\n",
        "    while not done:\r\n",
        "        # Pick epsilon-greedy action from possible actions for the current state\r\n",
        "        agent.set_epsilon(episode)\r\n",
        "        \r\n",
        "        # Agent selecting trip/action on current state\r\n",
        "        (action_index, action) = agent.get_action(curr_state)\r\n",
        "        \r\n",
        "        # Evaluate your reward and next state\r\n",
        "        (next_state, reward, total_worked_hours) = env.step(curr_state, action, Time_matrix)\r\n",
        "        \r\n",
        "        # updating agent total time journey with time taken for each trip\r\n",
        "        agent_total_time += total_worked_hours\r\n",
        "\r\n",
        "        # check if agent total drive is over defined 30 days. This will be end of episode\r\n",
        "        if agent_total_time >= max_car_drive_time:\r\n",
        "            done = True\r\n",
        "        \r\n",
        "        # append the experience to the memory\r\n",
        "        agent.append_sample(curr_state, action_index, reward, next_state, done)\r\n",
        "        \r\n",
        "        # train the model and also get the loss\r\n",
        "        nn_loss = agent.train_model()\r\n",
        "\r\n",
        "        # keep adding reward for each trip\r\n",
        "        total_reward += reward\r\n",
        "        \r\n",
        "        # preparing current state to next state for the next \r\n",
        "        curr_state = next_state\r\n",
        "\r\n",
        "\r\n",
        "    \"\"\" Keeping track to require data\"\"\"\r\n",
        "    # keeping track to rewards. We would be evaluating total reward with respect to 30*24\r\n",
        "    # rather than complete agent total time as it would be different in different cases. \r\n",
        "    # Aim is to get stablise the rewards in 30 days.\r\n",
        "    total_reward = (total_reward/ agent_total_time) * max_car_drive_time\r\n",
        "    rewards_per_episode.append(total_reward)\r\n",
        "    \r\n",
        "    # keeping track of neural network loss every episode\r\n",
        "    nn_loss_per_episode.append(nn_loss)\r\n",
        "    \r\n",
        "    # keeping track of q-value for state-action pair defined in sample_q_values\r\n",
        "    save_tracking_states(env, agent, state_size)\r\n",
        "\r\n",
        "    \r\n",
        "    # save weights every 1000 episode\r\n",
        "    if episode % 1000 == 0:\r\n",
        "        agent.save_model_weights(name=\"model_weights.h5\")\r\n",
        "    \r\n",
        "    # display the current process and metrics\r\n",
        "    if episode % 500 == 0:\r\n",
        "        print('Episdoe:{} NN_loss:{} Reward:{} Epsilon:{}'.format(\r\n",
        "            episode, agent_total_time, nn_loss, total_reward, agent.get_epsilon()))\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "# save rewards, nn_loss and tracked q-value\r\n",
        "save_obj(rewards_per_episode,'Rewards_per_Episode')\r\n",
        "save_obj(nn_loss_per_episode,'NN_Loss_per_Episode')  \r\n",
        "save_obj(States_track,'States_track')\r\n",
        "\r\n",
        "\r\n",
        "elapsed_time = time.time() - start_time\r\n",
        "print(\"Training completed. Time taken:[{}]\".format(elapsed_time))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes:   0%|          | 3/1500 [00:00<02:40,  9.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episdoe:0 NN_loss:731.0 Reward:None Epsilon:-9.849521203830369\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 12.8270\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.4552\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.3727\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.4574\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5097\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.9023\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.3527\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.8981\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4242\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1157\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9921\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0269\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6845\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4299\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.3541\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3206\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.2976\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.1779\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2223\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1953\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3659\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.5095\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.3999\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0763\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3039\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.8560\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.9599\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.6883\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.8572\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.5569\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.1329\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.9622\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.3919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Episodes:   0%|          | 4/1500 [00:07<44:58,  1.80s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-ca9809ba52af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# train the model and also get the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mnn_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# keep adding reward for each trip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-13-f6dc96e2faa3>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# predict the target q-values from states s\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;31m# target for q-network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Repositories\\Upgrad-PGD-ML---AI\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "metadata": {
        "id": "8PJ2kSRoVu_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212d67c7-42be-4727-ae89-f7cacbce1195"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Tracking Convergence"
      ],
      "metadata": {
        "id": "_76T2IABVu_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the states tracked for Q-values convergence"
      ],
      "metadata": {
        "id": "hSQSZyOFVu_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let check the convergence of state tracked q-value during training"
      ],
      "metadata": {
        "id": "4JL_DXmoVu_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "with open('States_track.pkl', 'rb') as handle:\r\n",
        "    States_track = pickle.load(handle)"
      ],
      "outputs": [],
      "metadata": {
        "id": "L4qXpWgvVu_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "for key, value in States_track.items():\r\n",
        "    for k,v in value.items():\r\n",
        "        print('[', key, k, ']:', len(v))"
      ],
      "outputs": [],
      "metadata": {
        "id": "iNRPQBZuVu_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef54577f-5145-4289-e172-933b9c43ca15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Lets plot the convergence of Q-value state-action pair\r\n",
        "\r\n",
        "plt.figure(0, figsize=(16,7))\r\n",
        "\r\n",
        "# plot 1\r\n",
        "plt.title('State:' + str(sample_q_values[0][0]) + ' ' +  'Action:' + str(sample_q_values[0][1]))\r\n",
        "plt.xlabel('Index')\r\n",
        "plt.ylabel('Q-value')\r\n",
        "plt.plot(\r\n",
        "    np.asarray(range(0, len(States_track[sample_q_values[0][0]].get(sample_q_values[0][1])[::10]))),\r\n",
        "    np.asarray(States_track[sample_q_values[0][0]].get(sample_q_values[0][1])[::10]))\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# plot 2\r\n",
        "plt.figure(0, figsize=(16,7))\r\n",
        "plt.title('State:' + str(sample_q_values[1][0]) + ' ' +  'Action:' + str(sample_q_values[1][1]))\r\n",
        "plt.xlabel('Index')\r\n",
        "plt.ylabel('Q-value')\r\n",
        "plt.plot(\r\n",
        "    np.asarray(range(0, len(States_track[sample_q_values[1][0]].get(sample_q_values[1][1])[::10]))),\r\n",
        "    np.asarray(States_track[sample_q_values[1][0]].get(sample_q_values[1][1])[::10]))\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "G5di2gsHVu_K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "outputId": "4d82db1c-172b-408c-b55c-7ceb599e6a61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the plots clearly indicate that it has converged and q-value is within a small margin. "
      ],
      "metadata": {
        "id": "1aYEBF8LVu_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the NN loss convergence"
      ],
      "metadata": {
        "id": "ohaMLmtIVu_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let check the convergence of Neural Network loss difference during training"
      ],
      "metadata": {
        "id": "8K1ZAtiuVu_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load the pickle file\r\n",
        "with open('NN_Loss_per_Episode.pkl', 'rb') as handle:\r\n",
        "    NN_Loss_per_Episode = pickle.load(handle)"
      ],
      "outputs": [],
      "metadata": {
        "id": "N7USaIrMVu_L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(0, figsize=(16,7))\r\n",
        "plt.title('Rewards per episode')\r\n",
        "xaxis = np.asarray(range(0, len(NN_Loss_per_Episode[::20])))\r\n",
        "plt.plot(xaxis,np.asarray(NN_Loss_per_Episode[::20]))\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "bXq4QvK5Vu_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "121d76b8-2ffb-4e55-cf6e-9364de8578fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above plot clearly indicates epsilon-greedy way of choosing either random action or based on max q-value.\n",
        "\n",
        "- Initial episodes Neural network is expected to have more loss because we wanted our model to explore more during start of initial training.\n",
        "- In later half of training, we can observe that neural network has converged, it is below ~.01. This happened because we wanted to exploit and try to find best action from a given state."
      ],
      "metadata": {
        "id": "srVLpmCbVu_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the Reward convergence"
      ],
      "metadata": {
        "id": "5cR6hyG1Vu_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let check the convergence of Rewards achieved per episode during training"
      ],
      "metadata": {
        "id": "cLCnVVTOVu_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# load the pickle file\r\n",
        "with open('Rewards_per_Episode.pkl', 'rb') as handle:\r\n",
        "    Rewards_per_Episode = pickle.load(handle)"
      ],
      "outputs": [],
      "metadata": {
        "id": "WdPRhNjKVu_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.figure(0, figsize=(16,7))\r\n",
        "plt.title('Rewards per episode')\r\n",
        "xaxis = np.asarray(range(0, len(Rewards_per_Episode[::10])))\r\n",
        "plt.plot(xaxis,np.asarray(Rewards_per_Episode[::10]))\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "fcSu8Ir1Vu_N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "fee81b21-4b7f-40e3-eefa-49594ad68a43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding convergence for reward is quite difficult. Because of following reasons:\n",
        "\n",
        "1. Initial state of agent(driver) is random. He can start from any location, any time, and day of week.\n",
        "2. Chossing a right ride based on q-value. (Possible rides are also random)\n",
        "3. Ride time also varies from location to location. Time and week days plays a major role of randomless.\n",
        "\n",
        "Because of these factors, rewards hasn't converged to expectation. <br/>\n",
        "But on an overall, Cab driver can efficiently get around 1800 to 2200 rewards as per above plot."
      ],
      "metadata": {
        "id": "B_tDgRXZVu_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Epsilon-decay sample function"
      ],
      "metadata": {
        "id": "qh5Fd-O_Vu_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "Try building a similar epsilon-decay function for your model.\n",
        "</div>"
      ],
      "metadata": {
        "id": "FItVU20LVu_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "time = np.arange(0,20000)\r\n",
        "epsilon = []\r\n",
        "for i in range(0,20000):\r\n",
        "    epsilon.append(0 + (1 - 0.01) * np.exp(-0.0005*i))"
      ],
      "outputs": [],
      "metadata": {
        "id": "XE0_VHJXVu_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plt.plot(time, epsilon)\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "pjYD1GONVu_O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "feca8169-2180-4d10-a8bb-9fd72cf0743e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "DRqHaItRVu_P"
      }
    }
  ]
}