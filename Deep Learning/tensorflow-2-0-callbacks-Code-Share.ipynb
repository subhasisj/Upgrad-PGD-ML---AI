{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Callbacks in Action\n",
    "\n",
    "\n",
    "So, what can you do with these callbacks?\n",
    "1. You can perform a particular task after the starting and ending of the training/batch/ epochs.\n",
    "2. You can periodically save the model states in the disk.\n",
    "3. You can schedule the learning rate as per your task.\n",
    "4. You can automatically stop the training when a particular condition becomes True.\n",
    "5. And you can do anything during the training process by subclassing these callbacks.\n",
    "\n",
    "\n",
    "\n",
    "Tensorflow provides a wide range of callbacks under the base class “tf.keras.callbacks. “For the full list of callbacks please visit [TensorFlow’s website](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback).\n",
    "\n",
    "\n",
    "\n",
    "1. custom callbacks by subclassing callback class.\n",
    "2. Early stopping callback.\n",
    "3. Model checkpoint callback.\n",
    "4. ReduceOnPlateu callback.\n",
    "5. Learning rate Scheduler.\n",
    "\n",
    "But let’s first load the cats_vs_dogs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 training data examples\n",
      "300 validation data examples\n",
      "300 test data examples\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "images_train = np.load(\"./images_train.npy\") / 255\n",
    "images_valid = np.load(\"./images_valid.npy\") / 255\n",
    "images_test = np.load(\"./images_test.npy\") / 255\n",
    "labels_train = np.load(\"./labels_train.npy\")\n",
    "labels_valid = np.load(\"./labels_valid.npy\")\n",
    "labels_test= np.load(\"./labels_test.npy\")\n",
    "\n",
    "print(\"{} training data examples\".format(images_train.shape[0]))\n",
    "print(\"{} validation data examples\".format(images_valid.shape[0]))\n",
    "print(\"{} test data examples\".format(images_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.10588235, 0.05098039, 0.        ],\n",
       "         [0.11372549, 0.05882353, 0.00784314],\n",
       "         [0.09019608, 0.03529412, 0.        ],\n",
       "         ...,\n",
       "         [0.17254902, 0.10588235, 0.00392157],\n",
       "         [0.19215686, 0.1254902 , 0.02352941],\n",
       "         [0.20392157, 0.1372549 , 0.03529412]],\n",
       "\n",
       "        [[0.10196078, 0.04705882, 0.        ],\n",
       "         [0.11764706, 0.0627451 , 0.01176471],\n",
       "         [0.10196078, 0.04705882, 0.00392157],\n",
       "         ...,\n",
       "         [0.2       , 0.13333333, 0.03137255],\n",
       "         [0.21568627, 0.14901961, 0.04705882],\n",
       "         [0.22352941, 0.15686275, 0.05490196]],\n",
       "\n",
       "        [[0.08627451, 0.03137255, 0.        ],\n",
       "         [0.10196078, 0.04705882, 0.        ],\n",
       "         [0.09411765, 0.03921569, 0.        ],\n",
       "         ...,\n",
       "         [0.19215686, 0.1254902 , 0.02352941],\n",
       "         [0.20392157, 0.1372549 , 0.03529412],\n",
       "         [0.21176471, 0.14509804, 0.04313725]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.30196078, 0.21568627, 0.07058824],\n",
       "         [0.30980392, 0.22352941, 0.07843137],\n",
       "         [0.31764706, 0.23137255, 0.08627451],\n",
       "         ...,\n",
       "         [0.3254902 , 0.25098039, 0.09411765],\n",
       "         [0.33333333, 0.25882353, 0.10196078],\n",
       "         [0.33333333, 0.25882353, 0.10196078]],\n",
       "\n",
       "        [[0.28627451, 0.2       , 0.05490196],\n",
       "         [0.30196078, 0.21568627, 0.07058824],\n",
       "         [0.32941176, 0.24313725, 0.09803922],\n",
       "         ...,\n",
       "         [0.32156863, 0.24705882, 0.09019608],\n",
       "         [0.3372549 , 0.2627451 , 0.10588235],\n",
       "         [0.34117647, 0.26666667, 0.10980392]],\n",
       "\n",
       "        [[0.30196078, 0.21568627, 0.07058824],\n",
       "         [0.30588235, 0.21960784, 0.0745098 ],\n",
       "         [0.30980392, 0.22352941, 0.07843137],\n",
       "         ...,\n",
       "         [0.30980392, 0.24313725, 0.0745098 ],\n",
       "         [0.3372549 , 0.27058824, 0.10196078],\n",
       "         [0.34509804, 0.27843137, 0.10980392]]],\n",
       "\n",
       "\n",
       "       [[[0.96862745, 0.97254902, 0.94117647],\n",
       "         [0.93333333, 0.9372549 , 0.90588235],\n",
       "         [0.69803922, 0.70196078, 0.67058824],\n",
       "         ...,\n",
       "         [0.54901961, 0.58039216, 0.5372549 ],\n",
       "         [0.5372549 , 0.57647059, 0.54117647],\n",
       "         [0.49019608, 0.49019608, 0.45882353]],\n",
       "\n",
       "        [[0.80392157, 0.80784314, 0.77647059],\n",
       "         [0.80784314, 0.81176471, 0.78039216],\n",
       "         [0.63137255, 0.63529412, 0.60392157],\n",
       "         ...,\n",
       "         [0.55294118, 0.58039216, 0.55294118],\n",
       "         [0.53333333, 0.57647059, 0.55294118],\n",
       "         [0.50588235, 0.50980392, 0.47843137]],\n",
       "\n",
       "        [[0.98039216, 0.98431373, 0.96078431],\n",
       "         [0.97647059, 0.98039216, 0.95686275],\n",
       "         [0.82352941, 0.82745098, 0.80392157],\n",
       "         ...,\n",
       "         [0.56078431, 0.59607843, 0.58431373],\n",
       "         [0.54509804, 0.58431373, 0.58039216],\n",
       "         [0.5254902 , 0.52941176, 0.50588235]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         ...,\n",
       "         [0.61568627, 0.63529412, 0.61960784],\n",
       "         [0.58823529, 0.60784314, 0.59215686],\n",
       "         [0.58431373, 0.60392157, 0.58823529]],\n",
       "\n",
       "        [[0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         ...,\n",
       "         [0.61568627, 0.63529412, 0.61960784],\n",
       "         [0.58823529, 0.60784314, 0.59215686],\n",
       "         [0.58431373, 0.60392157, 0.58823529]],\n",
       "\n",
       "        [[0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         [0.56862745, 0.58823529, 0.57254902],\n",
       "         ...,\n",
       "         [0.61568627, 0.63529412, 0.61960784],\n",
       "         [0.58823529, 0.60784314, 0.59215686],\n",
       "         [0.58431373, 0.60392157, 0.58823529]]],\n",
       "\n",
       "\n",
       "       [[[0.07843137, 0.09411765, 0.10588235],\n",
       "         [0.09411765, 0.11372549, 0.1254902 ],\n",
       "         [0.11764706, 0.14117647, 0.14117647],\n",
       "         ...,\n",
       "         [0.63529412, 0.56078431, 0.5372549 ],\n",
       "         [0.72156863, 0.63529412, 0.64705882],\n",
       "         [0.77647059, 0.69019608, 0.70196078]],\n",
       "\n",
       "        [[0.08627451, 0.10196078, 0.11372549],\n",
       "         [0.10588235, 0.1254902 , 0.1372549 ],\n",
       "         [0.12941176, 0.15294118, 0.15294118],\n",
       "         ...,\n",
       "         [0.61176471, 0.5372549 , 0.51372549],\n",
       "         [0.66666667, 0.58823529, 0.59607843],\n",
       "         [0.62745098, 0.54901961, 0.55686275]],\n",
       "\n",
       "        [[0.09803922, 0.11372549, 0.1254902 ],\n",
       "         [0.11764706, 0.1372549 , 0.14901961],\n",
       "         [0.1372549 , 0.16078431, 0.16078431],\n",
       "         ...,\n",
       "         [0.59607843, 0.52156863, 0.49803922],\n",
       "         [0.58823529, 0.5254902 , 0.52941176],\n",
       "         [0.45098039, 0.38823529, 0.39215686]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.02745098, 0.02745098, 0.02745098],\n",
       "         [0.03529412, 0.03529412, 0.03529412],\n",
       "         [0.05098039, 0.05098039, 0.05098039],\n",
       "         ...,\n",
       "         [0.18039216, 0.14509804, 0.21960784],\n",
       "         [0.14117647, 0.10980392, 0.16078431],\n",
       "         [0.14117647, 0.10980392, 0.16078431]],\n",
       "\n",
       "        [[0.01960784, 0.01960784, 0.01960784],\n",
       "         [0.02745098, 0.02745098, 0.02745098],\n",
       "         [0.04705882, 0.04705882, 0.04705882],\n",
       "         ...,\n",
       "         [0.16078431, 0.1254902 , 0.2       ],\n",
       "         [0.19607843, 0.16470588, 0.21568627],\n",
       "         [0.18039216, 0.14901961, 0.2       ]],\n",
       "\n",
       "        [[0.03137255, 0.03137255, 0.03137255],\n",
       "         [0.03529412, 0.03529412, 0.03529412],\n",
       "         [0.03137255, 0.03137255, 0.03137255],\n",
       "         ...,\n",
       "         [0.16470588, 0.13333333, 0.19215686],\n",
       "         [0.17254902, 0.14117647, 0.19215686],\n",
       "         [0.17254902, 0.14117647, 0.19215686]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.61568627, 0.63137255, 0.39215686],\n",
       "         [0.6745098 , 0.65098039, 0.42352941],\n",
       "         [0.76078431, 0.69411765, 0.48235294],\n",
       "         ...,\n",
       "         [0.68235294, 0.6627451 , 0.43529412],\n",
       "         [0.65882353, 0.63529412, 0.40784314],\n",
       "         [0.6745098 , 0.63137255, 0.41176471]],\n",
       "\n",
       "        [[0.63137255, 0.63137255, 0.39607843],\n",
       "         [0.70980392, 0.66666667, 0.44705882],\n",
       "         [0.7372549 , 0.65490196, 0.44705882],\n",
       "         ...,\n",
       "         [0.60784314, 0.58823529, 0.36078431],\n",
       "         [0.60392157, 0.58039216, 0.35294118],\n",
       "         [0.61568627, 0.58039216, 0.35686275]],\n",
       "\n",
       "        [[0.66666667, 0.64705882, 0.41960784],\n",
       "         [0.67843137, 0.62745098, 0.41176471],\n",
       "         [0.7254902 , 0.64313725, 0.43529412],\n",
       "         ...,\n",
       "         [0.63529412, 0.61568627, 0.38823529],\n",
       "         [0.59215686, 0.56862745, 0.34117647],\n",
       "         [0.63137255, 0.61176471, 0.38431373]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.68627451, 0.63921569, 0.43529412],\n",
       "         [0.73333333, 0.70588235, 0.49411765],\n",
       "         [0.72941176, 0.71764706, 0.50196078],\n",
       "         ...,\n",
       "         [0.75686275, 0.68627451, 0.49019608],\n",
       "         [0.88627451, 0.8       , 0.61568627],\n",
       "         [0.69019608, 0.57647059, 0.40392157]],\n",
       "\n",
       "        [[0.5254902 , 0.4745098 , 0.29803922],\n",
       "         [0.77647059, 0.76862745, 0.57647059],\n",
       "         [0.51764706, 0.53333333, 0.33333333],\n",
       "         ...,\n",
       "         [0.57254902, 0.49411765, 0.29411765],\n",
       "         [0.42352941, 0.39607843, 0.18039216],\n",
       "         [0.55686275, 0.49019608, 0.28627451]],\n",
       "\n",
       "        [[0.72941176, 0.67843137, 0.50196078],\n",
       "         [0.56862745, 0.56078431, 0.36862745],\n",
       "         [0.54901961, 0.56470588, 0.36470588],\n",
       "         ...,\n",
       "         [0.58431373, 0.50588235, 0.30588235],\n",
       "         [0.67058824, 0.64313725, 0.42745098],\n",
       "         [0.63529412, 0.56862745, 0.36470588]]],\n",
       "\n",
       "\n",
       "       [[[0.56470588, 0.60392157, 0.61176471],\n",
       "         [0.57647059, 0.61568627, 0.62352941],\n",
       "         [0.58823529, 0.62745098, 0.63529412],\n",
       "         ...,\n",
       "         [0.74509804, 0.7254902 , 0.75294118],\n",
       "         [0.74117647, 0.72156863, 0.74901961],\n",
       "         [0.73333333, 0.71372549, 0.74117647]],\n",
       "\n",
       "        [[0.57254902, 0.61176471, 0.61960784],\n",
       "         [0.58431373, 0.62352941, 0.63137255],\n",
       "         [0.6       , 0.63921569, 0.64705882],\n",
       "         ...,\n",
       "         [0.74117647, 0.72941176, 0.75686275],\n",
       "         [0.7372549 , 0.7254902 , 0.75294118],\n",
       "         [0.72941176, 0.71764706, 0.74509804]],\n",
       "\n",
       "        [[0.58039216, 0.61960784, 0.62745098],\n",
       "         [0.59215686, 0.63137255, 0.63921569],\n",
       "         [0.60784314, 0.64705882, 0.65490196],\n",
       "         ...,\n",
       "         [0.74117647, 0.7372549 , 0.76078431],\n",
       "         [0.7372549 , 0.73333333, 0.75686275],\n",
       "         [0.72941176, 0.7254902 , 0.74901961]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.28627451, 0.30588235, 0.31764706],\n",
       "         [0.2627451 , 0.28235294, 0.29411765],\n",
       "         [0.23921569, 0.25490196, 0.25882353],\n",
       "         ...,\n",
       "         [0.09411765, 0.15294118, 0.18039216],\n",
       "         [0.09411765, 0.15294118, 0.18039216],\n",
       "         [0.09019608, 0.14901961, 0.17647059]],\n",
       "\n",
       "        [[0.21176471, 0.23137255, 0.24313725],\n",
       "         [0.18823529, 0.20784314, 0.21960784],\n",
       "         [0.18431373, 0.2       , 0.20392157],\n",
       "         ...,\n",
       "         [0.09019608, 0.14901961, 0.17647059],\n",
       "         [0.09019608, 0.14901961, 0.17647059],\n",
       "         [0.08627451, 0.14509804, 0.17254902]],\n",
       "\n",
       "        [[0.21176471, 0.20784314, 0.19215686],\n",
       "         [0.22352941, 0.21960784, 0.20392157],\n",
       "         [0.21176471, 0.20784314, 0.19215686],\n",
       "         ...,\n",
       "         [0.10196078, 0.14117647, 0.17647059],\n",
       "         [0.10196078, 0.14117647, 0.17647059],\n",
       "         [0.09803922, 0.1372549 , 0.17254902]]],\n",
       "\n",
       "\n",
       "       [[[0.85882353, 0.79215686, 0.75294118],\n",
       "         [0.85882353, 0.79215686, 0.76078431],\n",
       "         [0.84313725, 0.77647059, 0.74901961],\n",
       "         ...,\n",
       "         [1.        , 0.97254902, 0.93333333],\n",
       "         [1.        , 0.97254902, 0.93333333],\n",
       "         [1.        , 0.97254902, 0.93333333]],\n",
       "\n",
       "        [[0.85098039, 0.78431373, 0.74509804],\n",
       "         [0.85490196, 0.78823529, 0.75686275],\n",
       "         [0.84313725, 0.77647059, 0.74901961],\n",
       "         ...,\n",
       "         [0.99607843, 0.96862745, 0.92941176],\n",
       "         [0.99607843, 0.96862745, 0.92941176],\n",
       "         [1.        , 0.97254902, 0.93333333]],\n",
       "\n",
       "        [[0.84313725, 0.77647059, 0.7372549 ],\n",
       "         [0.85490196, 0.78823529, 0.75686275],\n",
       "         [0.84705882, 0.78039216, 0.75294118],\n",
       "         ...,\n",
       "         [0.99215686, 0.96470588, 0.9254902 ],\n",
       "         [0.99215686, 0.96470588, 0.9254902 ],\n",
       "         [1.        , 0.97254902, 0.93333333]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.71764706, 0.67058824, 0.95294118],\n",
       "         [0.70588235, 0.67058824, 0.9254902 ],\n",
       "         [0.6745098 , 0.65490196, 0.88235294],\n",
       "         ...,\n",
       "         [0.39215686, 0.32156863, 0.28235294],\n",
       "         [0.39215686, 0.31372549, 0.27843137],\n",
       "         [0.41176471, 0.31372549, 0.29803922]],\n",
       "\n",
       "        [[0.75294118, 0.70196078, 0.90980392],\n",
       "         [0.74509804, 0.70588235, 0.91372549],\n",
       "         [0.67843137, 0.64313725, 0.85882353],\n",
       "         ...,\n",
       "         [0.39607843, 0.3372549 , 0.31764706],\n",
       "         [0.42352941, 0.34117647, 0.32941176],\n",
       "         [0.43529412, 0.3372549 , 0.32156863]],\n",
       "\n",
       "        [[0.78039216, 0.72941176, 0.9372549 ],\n",
       "         [0.75294118, 0.71372549, 0.92156863],\n",
       "         [0.67058824, 0.63529412, 0.85098039],\n",
       "         ...,\n",
       "         [0.37647059, 0.31764706, 0.29803922],\n",
       "         [0.43529412, 0.35294118, 0.34117647],\n",
       "         [0.43137255, 0.33333333, 0.31764706]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cat Dog - Binary Classification Problem \n",
    "\n",
    "# CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 160, 160, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 160, 160, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 80, 80, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 102400)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               13107328  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 13,173,025\n",
      "Trainable params: 13,173,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D , MaxPool2D\n",
    "def get_compiled_model(compile=True):\n",
    "  ''' prepare and compile the model '''\n",
    "  model = Sequential()\n",
    "  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME', input_shape=(160,160,3)))\n",
    "  model.add(Conv2D(32, (3,3), activation='relu', padding='SAME'))\n",
    "  model.add(MaxPool2D(2,2))\n",
    "  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n",
    "  model.add(Conv2D(64, (3,3), activation='relu', padding='SAME'))\n",
    "  model.add(MaxPool2D(2,2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  \n",
    "  if compile is True:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(), \n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(name='acc')])\n",
    "  return model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "# inspecting the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Custom callbacks by subclassing callback class.\n",
    "\n",
    "These callbacks come under the base class “tf.keras.callbacks.”\n",
    "By subclassing these callbacks, we can perform certain functions when the training/batch/epochs have started or ended.\n",
    "For this, we can override the function of callback classes.\n",
    "The name of these functions is self explain their behavior.\n",
    "For example def on_train_begin(), this means what to do when\n",
    "training will begin.\n",
    "Let’s see below how to override these functions. We can\n",
    "also, monitor logs and perform certain actions, generally at \n",
    "the starting or the ending of the training/batch/epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is started, at time 11:11:32.048510\n",
      "Training: batch 0 begins at 11:11:32.070100\n",
      "Training: batch 0 ends at 11:11:37.553522\n",
      " 1/19 [>.............................] - ETA: 0s - loss: 0.6954 - acc: 0.5312Training: batch 1 begins at 11:11:37.558305\n",
      "Training: batch 1 ends at 11:11:37.600523\n",
      "Training: batch 2 begins at 11:11:37.600835\n",
      "Training: batch 2 ends at 11:11:37.636836\n",
      " 3/19 [===>..........................] - ETA: 0s - loss: 1.9636 - acc: 0.4688Training: batch 3 begins at 11:11:37.638112\n",
      "Training: batch 3 ends at 11:11:37.672230\n",
      "Training: batch 4 begins at 11:11:37.672842\n",
      "Training: batch 4 ends at 11:11:37.708426\n",
      " 5/19 [======>.......................] - ETA: 0s - loss: 1.4605 - acc: 0.5063Training: batch 5 begins at 11:11:37.709701\n",
      "Training: batch 5 ends at 11:11:37.741226\n",
      "Training: batch 6 begins at 11:11:37.741515\n",
      "Training: batch 6 ends at 11:11:37.772384\n",
      " 7/19 [==========>...................] - ETA: 0s - loss: 1.2474 - acc: 0.4955Training: batch 7 begins at 11:11:37.773704\n",
      "Training: batch 7 ends at 11:11:37.807318\n",
      "Training: batch 8 begins at 11:11:37.807915\n",
      "Training: batch 8 ends at 11:11:37.838598\n",
      " 9/19 [=============>................] - ETA: 0s - loss: 1.1237 - acc: 0.4896Training: batch 9 begins at 11:11:37.839708\n",
      "Training: batch 9 ends at 11:11:37.872714\n",
      "Training: batch 10 begins at 11:11:37.873090\n",
      "Training: batch 10 ends at 11:11:37.906007\n",
      "11/19 [================>.............] - ETA: 0s - loss: 1.0478 - acc: 0.4858Training: batch 11 begins at 11:11:37.907348\n",
      "Training: batch 11 ends at 11:11:37.941149\n",
      "Training: batch 12 begins at 11:11:37.941569\n",
      "Training: batch 12 ends at 11:11:37.977339\n",
      "13/19 [===================>..........] - ETA: 0s - loss: 0.9930 - acc: 0.4880Training: batch 13 begins at 11:11:37.978584\n",
      "Training: batch 13 ends at 11:11:38.011886\n",
      "Training: batch 14 begins at 11:11:38.012421\n",
      "Training: batch 14 ends at 11:11:38.044896\n",
      "15/19 [======================>.......] - ETA: 0s - loss: 0.9528 - acc: 0.4938Training: batch 15 begins at 11:11:38.046860\n",
      "Training: batch 15 ends at 11:11:38.081686\n",
      "Training: batch 16 begins at 11:11:38.082626\n",
      "Training: batch 16 ends at 11:11:38.116803\n",
      "17/19 [=========================>....] - ETA: 0s - loss: 0.9216 - acc: 0.5129Training: batch 17 begins at 11:11:38.119156\n",
      "Training: batch 17 ends at 11:11:38.152294\n",
      "Training: batch 18 begins at 11:11:38.153079\n",
      "Training: batch 18 ends at 11:11:38.397893\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 0.9009 - acc: 0.5083 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Training is ended at 11:11:39.021573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0978098c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "  def on_train_begin(self,logs=None):\n",
    "    print(\"Training is started, at time {}\".format(datetime.datetime.now().time()))\n",
    "  def on_train_end(self, logs=None):\n",
    "    print(\"Training is ended at {}\".format(datetime.datetime.now().time()))\n",
    "  def on_train_batch_begin(self, batch, logs=None):\n",
    "    print('Training: batch {} begins at {}'.format(batch, datetime.datetime.now().time()))\n",
    "  def on_train_batch_end(self, batch, logs=None):\n",
    "    print('Training: batch {} ends at {}'.format(batch, datetime.datetime.now().time()))\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(images_train, labels_train, validation_data=(images_valid, labels_valid), \n",
    "          epochs=1, callbacks=[custom_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. EarlyStopping Callback.\n",
    "\n",
    "\n",
    " EarlyStopping Callback.\n",
    "So, let’s see how one can use this callback.\n",
    "\n",
    "First, import the callback, and then create the instance of the\n",
    "EarlyStopping callback and pass the arguments as per our needs.\n",
    "\n",
    "* “monitor” you can pass the loss or the metric.\n",
    "Generally, we pass val_loss and monitor it.\n",
    "\n",
    "* “min_delta” you can pass an integer in this argument.\n",
    "In simple words, you’re telling the callback that the model\n",
    "is not improving if it’s not decreasing more/less than the loss/metrics.\n",
    "\n",
    "* “patience,” it means about how many epochs to wait.\n",
    "And after that, if there is no improvement seen in the\n",
    "model performance according to the value of “min delta,” then stop the training.\n",
    "\n",
    "* “mode”\n",
    "By default it’s set to ‘auto’ this comes handy when\n",
    "you’re dealing with the custom loss/metric. So, you can \n",
    "tell the callback whether the model is improving when\n",
    "its custom loss/metric is decreasing then set it to “min” \n",
    "or increasing then set it to “max.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 1.1642 - acc: 0.5100 - val_loss: 0.6916 - val_acc: 0.5033\n",
      "Epoch 2/80\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.6955 - acc: 0.5400 - val_loss: 0.6910 - val_acc: 0.5067\n",
      "Epoch 3/80\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.6760 - acc: 0.5833 - val_loss: 0.6640 - val_acc: 0.5933\n",
      "Epoch 4/80\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.6352 - acc: 0.6600 - val_loss: 0.7010 - val_acc: 0.5633\n",
      "Epoch 5/80\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.6196 - acc: 0.6433 - val_loss: 0.6550 - val_acc: 0.6300\n",
      "Epoch 6/80\n",
      "19/19 [==============================] - 1s 48ms/step - loss: 0.5265 - acc: 0.7467 - val_loss: 0.6948 - val_acc: 0.6067\n",
      "Epoch 7/80\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.4784 - acc: 0.7783 - val_loss: 0.6392 - val_acc: 0.6533\n",
      "Epoch 8/80\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.3323 - acc: 0.8467 - val_loss: 0.7749 - val_acc: 0.5967\n",
      "Epoch 9/80\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.1997 - acc: 0.9233 - val_loss: 1.0839 - val_acc: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0930241e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                               min_delta=0.001, \n",
    "                               patience=2, \n",
    "                               verbose=0, \n",
    "                               mode='min', \n",
    "                               baseline=None, \n",
    "                               restore_best_weights=False)\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(images_train, labels_train, \n",
    "          validation_data=(images_valid, labels_valid), \n",
    "          epochs=80, \n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReduceLROnPlateau.\n",
    "\n",
    "This callback is used to reduce the learning rate if there is \n",
    "not any improvement in the loss/metric.\n",
    "\n",
    "The arguments are:\n",
    "\n",
    "* “monitor” it’s set to that loss/metric as a string\n",
    " of which we are reducing the learning if it’ll not improve.\n",
    "\n",
    "* “factor” You can pass an integer in this argument,\n",
    "and say your current learning rate is LR, then if\n",
    "there is not any improvement seen in the monitored loss/metric,\n",
    "then the learning is going to decrease by that “factor.”\n",
    "i.e new learning rate = lr * factor\n",
    "\n",
    "* “Verbose”\n",
    "You can set verbose =1 to see the learning rate at every epoch.\n",
    "Or verbose = 0 to disable it.\n",
    "\n",
    "The argument min_delta and mode are the same as explained in the arguments of EarlyStopping Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 1s 54ms/step - loss: 1.0484 - acc: 0.4683 - val_loss: 0.7127 - val_acc: 0.5000 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.6907 - acc: 0.5400 - val_loss: 0.6909 - val_acc: 0.5333 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.6925 - acc: 0.5683 - val_loss: 0.6924 - val_acc: 0.5133 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 51ms/step - loss: 0.6754 - acc: 0.5717 - val_loss: 0.6829 - val_acc: 0.5867 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.6394 - acc: 0.6383 - val_loss: 0.6495 - val_acc: 0.6333 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.6362 - acc: 0.6667 - val_loss: 0.6937 - val_acc: 0.6000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.6010 - acc: 0.6750 - val_loss: 0.6417 - val_acc: 0.6300 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.4776 - acc: 0.7883 - val_loss: 0.6323 - val_acc: 0.6533 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.3271 - acc: 0.8667 - val_loss: 0.7970 - val_acc: 0.6200 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.2222 - acc: 0.9033 - val_loss: 1.0517 - val_acc: 0.6433 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0951 - acc: 0.9717 - val_loss: 1.3546 - val_acc: 0.6433 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 47ms/step - loss: 0.0456 - acc: 0.9883 - val_loss: 1.6510 - val_acc: 0.6367 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0184 - acc: 0.9933 - val_loss: 1.8593 - val_acc: 0.6500 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0111 - acc: 0.9983 - val_loss: 1.7777 - val_acc: 0.6500 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0145 - acc: 0.9967 - val_loss: 2.2447 - val_acc: 0.6400 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0518 - acc: 0.9767 - val_loss: 2.3223 - val_acc: 0.6267 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0270 - acc: 0.9933 - val_loss: 1.9019 - val_acc: 0.6300 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.0134 - acc: 0.9950 - val_loss: 1.9575 - val_acc: 0.6667 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.0239 - val_acc: 0.6733 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0899 - val_acc: 0.6700 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f08c87da8d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "callback  = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.1, \n",
    "                              patience=10, \n",
    "                              verbose=0, \n",
    "                              mode='auto', \n",
    "                              min_delta=0.002, \n",
    "                              cooldown=0, \n",
    "                              min_lr=0)\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(images_train, labels_train, \n",
    "          validation_data=(images_valid, labels_valid), \n",
    "          epochs=20, \n",
    "          callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ModelCheckpoint\n",
    "\n",
    "TO Save Model Checkpoint\n",
    "\n",
    "So, let’s see how we can use this callback. We can save\n",
    "the model checkpoint in Keras h5/hd5 format or TensorFlow pb\n",
    "format. If you pass the argument “filepath= model.h5”(.h5 extension)\n",
    "it’ll be saved in the Keras format or “filepath= model.p”(.pb extension)\n",
    "for saving in the TensorFlow model format.\n",
    "\n",
    "Also, there are two options to save the checkpoint either you can save the entire architecture+weights or just the weights. You can do this by setting “save_only_weights=True” or “save_only_weights=False”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 1s 75ms/step - loss: 1.1287 - acc: 0.4683 - val_loss: 0.6929 - val_acc: 0.5033\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6954 - acc: 0.5283 - val_loss: 0.6914 - val_acc: 0.5033\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6901 - acc: 0.5617 - val_loss: 0.6889 - val_acc: 0.5600\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.6762 - acc: 0.5917 - val_loss: 0.6854 - val_acc: 0.5633\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 3s 145ms/step - loss: 0.6333 - acc: 0.6650 - val_loss: 0.6594 - val_acc: 0.6133\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 70ms/step - loss: 0.5314 - acc: 0.7350 - val_loss: 0.6804 - val_acc: 0.5733\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 66ms/step - loss: 0.4225 - acc: 0.7967 - val_loss: 0.8223 - val_acc: 0.5900\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 0.3065 - acc: 0.8833 - val_loss: 1.0798 - val_acc: 0.5767\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.1593 - acc: 0.9333 - val_loss: 1.0745 - val_acc: 0.5933\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 68ms/step - loss: 0.1336 - acc: 0.9633 - val_loss: 1.5117 - val_acc: 0.5567\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 124ms/step - loss: 0.0715 - acc: 0.9783 - val_loss: 1.7069 - val_acc: 0.5833\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.0801 - acc: 0.9767 - val_loss: 1.4689 - val_acc: 0.5733\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 67ms/step - loss: 0.0371 - acc: 0.9900 - val_loss: 2.2223 - val_acc: 0.6067\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 2s 125ms/step - loss: 0.0115 - acc: 0.9983 - val_loss: 3.5619 - val_acc: 0.6067\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 65ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 3.1005 - val_acc: 0.5900\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 74ms/step - loss: 0.0596 - acc: 0.9867 - val_loss: 3.2702 - val_acc: 0.5700\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 0.0525 - acc: 0.9817 - val_loss: 2.6308 - val_acc: 0.5933\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 75ms/step - loss: 0.0784 - acc: 0.9883 - val_loss: 2.2343 - val_acc: 0.6000\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 69ms/step - loss: 0.0232 - acc: 0.9950 - val_loss: 2.3654 - val_acc: 0.5867\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.4344 - val_acc: 0.5900\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# ModelCheckpoint \n",
    "\n",
    "# Architecure \n",
    "# Weights \n",
    "\n",
    "\n",
    "model_checkpoint_callback = ModelCheckpoint(filepath= \"model.h5\", \n",
    "                                            monitor='val_loss', \n",
    "                                            verbose=0, \n",
    "                                            save_best_only=False, \n",
    "                                            save_weights_only=False, \n",
    "                                            mode='min', \n",
    "                                            save_freq='epoch')\n",
    "model = get_compiled_model()\n",
    "\n",
    "\n",
    "model.fit(images_train, labels_train, \n",
    "          validation_data=(images_valid, labels_valid), \n",
    "          epochs=20, \n",
    "          callbacks=[model_checkpoint_callback])\n",
    "\n",
    "\n",
    "\n",
    "# Epoch 500 \n",
    "\n",
    "# Download \n",
    "\n",
    "# loading the model from the disk.\n",
    "model.load_weights(\"model.h5\") # Loading your Model FIle .h5 keras , Tensor pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_16 (Conv2D)           (None, 160, 160, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 160, 160, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 80, 80, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 80, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 102400)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               13107328  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 13,173,025\n",
      "Trainable params: 13,173,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# inspecting the architecture .\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 12ms/step - loss: 4.3676 - acc: 0.5433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.367551326751709, 0.5433333516120911]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model on the test set.\n",
    "model.evaluate(images_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LearningRateScheduler\n",
    ">  The simplest way to schedule the learning is to decrease the learning rate \n",
    "linearly from a large initial value to a small value. \n",
    "This allows large weight changes at the beginning of the \n",
    "the learning process and small changes or fine-tuning towards\n",
    "the end of the learning process.\n",
    "\n",
    "Let’s see how to schedule the learning rate. For this, we have to\n",
    "define an auxiliary function that contains the rules for\n",
    "alternating the learning rate. \n",
    "And then we can simply pass the name of this auxiliary function\n",
    "to the argument of the object of the LearningRateScheduler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n",
      "Epoch 1/10\n",
      "19/19 [==============================] - 1s 52ms/step - loss: 1.1175 - acc: 0.5067 - val_loss: 0.6925 - val_acc: 0.5000 - lr: 0.0010\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.6919 - acc: 0.4950 - val_loss: 0.6889 - val_acc: 0.5133 - lr: 0.0020\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.6895 - acc: 0.5433 - val_loss: 0.6864 - val_acc: 0.5000 - lr: 0.0020\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.6917 - acc: 0.5100 - val_loss: 0.6913 - val_acc: 0.5000 - lr: 0.0050\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.7033 - acc: 0.5283 - val_loss: 0.6934 - val_acc: 0.5000 - lr: 0.0050\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.6952 - acc: 0.5100 - val_loss: 0.9501 - val_acc: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 2.2907 - acc: 0.4850 - val_loss: 0.6933 - val_acc: 0.5000 - lr: 0.0100\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 1s 46ms/step - loss: 0.7325 - acc: 0.5100 - val_loss: 0.6939 - val_acc: 0.5000 - lr: 0.0170\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 0.6923 - acc: 0.4933 - val_loss: 0.7938 - val_acc: 0.5000 - lr: 0.0170\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.8891 - acc: 0.4683 - val_loss: 0.6932 - val_acc: 0.5000 - lr: 0.0260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0978b32050>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "def lr_function(epoch, lr):\n",
    "    if epoch % 2 == 0:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr + epoch/1000\n",
    "\n",
    "learning_rate_schedular_callback = LearningRateScheduler(schedule= lr_function ,\n",
    "                                                         verbose=1)\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(images_train, labels_train, \n",
    "          validation_data=(images_valid, labels_valid), \n",
    "          epochs=10, \n",
    "          callbacks=[learning_rate_schedular_callback] )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
